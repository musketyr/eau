#summary Quantifying Assignments
#labels EarlyDraft,Featured
#sidebar TableOfContents

<wiki:toc max_depth="3" />

= Quantifying assignments =


Nowadays there is plenty of `*`Unit, Test`*` and other testing frameworks which 
helps us to test nearly everything. There are also a lot of tools like continuous 
integration servers which helps us to report their results. But there is one 
important problem.  Are they all tests equal?

----
====Example Assignment====
_*Implement vehicle based on given interface. The following features will be scored:*_
|| *Points* || *Requirements* ||
|| 4 || vehicle must change its direction appropriately when turning left or right ||
|| *Points* || *Bonus features* ||
|| 1 || vehicle might show proper blinker before and during turning left or right || 

----

Here we have desired interface:
====acme/Vehicle.java====
{{{
public interface Vehicle {
    Direction getDirection();
    void turnLeft();
    void turnRight();
    void prepareTurnLeft();
    void prepareTurnRight();
    boolean isLeftBlinkerOn();
    boolean isRightBlinkerOn();
}
}}}

And here is one of final works. The veteran car.

====acme/!VeteranCar.java====
{{{
public class VeteranCar implements Vehicle{     
    
    private Direction direction = Direction.NORTH;  
    
    @Override public Direction getDirection() { return direction; }
    @Override public boolean isLeftBlinkerOn() { return false; }
    @Override public boolean isRightBlinkerOn() { return false; }
    @Override public void prepareTurnLeft() {}  
    @Override public void prepareTurnRight() {}
    @Override public void turnLeft() { direction = direction.left(); }
    @Override public void turnRight() { direction = direction.right(); }  
}
}}}
As we see blinkers are not supported by this car so the author will get no extra points
but still she should get all required points.

We can write a simple test to check whether student's implementation works as it 
should. It could look like one this:

====acme/!VehicleTest01.java====
{{{
public class VehicleTest01 {
    protected Vehicle fixture;
    
    // setup and other tests
    
    @Test public void testTurnRight(){
		Direction original = fixture.getDirection();
		fixture.turnRight();
		assertEquals(original.right(), fixture.getDirection());
    }
    
    @Test public void testPrepareRight(){
		Direction original = fixture.getDirection();
		fixture.prepareTurnRight();
		assertEquals(original, fixture.getDirection());
		assertTrue(fixture.isRightBlinkerOn());
		assertFalse(fixture.isLeftBlinkerOn());
    }
}
}}}

And here is the problem. Importance of {{{testTurnRight}}} is much higher than 
{{{testPrepareRight}}}, but there is no sign about this in the code. So there is no 
way how to find out how many points does particular student get until we match 
test results with the original assignment. But there can be some better way. We 
can make special annotations which will help us to have everything on one place. 
Look on next code snippet. The points are divided by two because we can turn into
two directions.

====acme/!VehicleTest02.java====
{{{
public class VehicleTest02 {

    protected Vehicle fixture;
    
    // setup and other tests
    
    @Points(2)
    @Test public void testTurnRight(){
		Direction original = fixture.getDirection();
		fixture.turnRight();
		assertEquals(original.right(), fixture.getDirection());
    }
    
    @Points(0.5)
    @Test public void testPrepareRight(){
		Direction original = fixture.getDirection();
		fixture.prepareTurnRight();
		assertEquals(original, fixture.getDirection());
		assertTrue(fixture.isRightBlinkerOn());
		assertFalse(fixture.isLeftBlinkerOn());
    }   
}
}}}

We can even provide lot of more useful informations like tasks description and
that some test is testing bonus feature. Bonus feature is as expected the one which
gives some extra points.

====acme/!VehicleTest03.java====
{{{
public class VehicleTest03 {

    protected Vehicle fixture;
    
    // setup and other tests
    
    @Points(2)
    @Description("Vehicle changes direction to proper one after turning right")
    @Test public void testTurnRight(){
		Direction original = fixture.getDirection();
		fixture.turnRight();
		assertEquals(original.right(), fixture.getDirection());
    }
    
    @Bonus @Points(0.5)
    @Description("Vehicle blinks right when preparing to turn right")
    @Details("Vehicle must keep original direction when preparing to turn")
    @Test public void testPrepareRight(){
		Direction original = fixture.getDirection();
		fixture.prepareTurnRight();
		assertEquals(original, fixture.getDirection());
		assertTrue(fixture.isRightBlinkerOn());
		assertFalse(fixture.isLeftBlinkerOn());
    }
}
}}}

Usage of this annotations is quite straightforward. They all can be found in
{{{eu.ebdit.eau}}} package of [http://code.google.com/p/eau EAU project]. They 
can be used at method or class level. By annotating single test we say that
if this test is successful author should get particular number of points. By annotating
whole test class we say that some points should be given to author when all tests passes.

For example plane must be able to take off and also land safely:

====acme/!PlaneTest.java====
{{{
@Points(10)
@Description("Plane must take off and land safely!")
public class PlaneTest {
	@Test void testTakeOff(){...}
	@Test void testLand(){...}
}
}}}

Following table describes basic usage of annotations from {{{eu.ebdit.eau}}} package:

====Annotations in eu.ebdit.eau package====

|| *Annotation* || *Value* || *Usage* ||
|| *Bonus* || _None_ ||Marks test as optional. Failing the test will author not loose any important points but can gain extra ones.||
|| *Description* ||Short and meaningful description of test||Provide short and meaningful description for given test. This description will be shown in evaluation report.||
|| *Details* ||Additional details about the test||Provide additional details about the test. This details can be used for evaluating tools to provide more information about the test.||
|| *Points* ||Number of points if successful||Provide number of points for evaluation. The number can be any {{{double}}}. Negative numbers will be converted to their positive counterparts using absolute value. Zero value should not be used since there is no meaningful how to decide whether the test was successful after evaluation. *This annotation must be always present on test method or class otherwise the method or class will not be taken into account during evaluation!*||

Since we have annotated test we can also simply evaluate it. Imagine having test
{{{acme.VeteranCarTest}}} for ours {{{acme.VeteranCar}}} class shown above. 
Only thing we need is to download latest {{{eau-all}}} jar file from  
[http://code.google.com/p/eau/downloads/list?q=label:Featured EAU downloads web site].
The "all" jar file includes all dependencies for eau project so we don't need to bother
with anything else. 

The most primitive way how tp evaluate the test is just open command line 
and type following:

====Example command line run with acme.!VeteranCarTest====

{{{
java -cp "<path-to-eau-all.jar>;<path-to-your-project's-classpath>" eu.ebdit.eau.util.TextUI acme.VeteranCarTest
}}}

This is another real life example. The context directory of command shell is root directory of our compiled classes.
Underlaying platform is MS Windows and {{{eal-all}}} jar is located at {{{c:\eau\eau-all.0.0.1-0910102015.jar}}}.

====Real life command line example run with acme.VeteranCarTest====

{{{
java -cp "c:\eau\eau-all.0.0.1-0910102015.jar;." eu.ebdit.eau.util.TextUI acme.VeteranCarTest
}}}

After running this command we should get some logging messages and also our desired
evaluation report. It should look like this one:

====Text output from example run====

{{{
100,00% Test Reporter
        points: 2.0/2.0(2.5)
   100,00% Vehicle changes direction to proper one after turning right
           points: 2.0/2.0
     0,00% Vehicle blinks right when preparing to turn right
           points: 0.0/0.0(0.5)
}}}

The percentage number is called "success percentage" which is quite explaining.
The number means how successful was the evaluation. Note that bonus tests are not
counted so you can even be more than 100% successful.

Points shows number of points obtained followed by maximal number of points which
could be obtained (not counting bonus ones). If there are some bonus point the maximal
amount of point which can be obtained including bonus ones is written into parenthesis
after as the last number.

Sometime is also useful to have result in more format way such as XML. This is easy
using custom printers. EAU project comes with two predefined printers text printer
shown above and XML printer. You can also define your own printers implementing 
{{{eu.ebdit.eau.Printer}}} interface. To enable custom printer pass its class name
as {{{-p}}} parameter to {{{eu.ebdit.eau.TextUI}}} main class. Note you can also forward
output to specific file using standard {{{>}}} command char.

====Example command line run with acme.!VeteranCarTest with XML output====

{{{
java -cp "<path-to-eau-all.jar>;<path-to-your-project's-classpath>" eu.ebdit.eau.util.TextUI -p eu.ebdit.eau.util.XmlReportPrinter acme.VeteranCarTest > result.xml
}}}

The result should look like following listing:

====XML output from example run====

{{{
<report>
  <description>Test Reporter</description>
  <successPercentage>1.0</successPercentage>
  <points>2.0</points>
  <maxPoints>2.0</maxPoints>
  <maxPointsWithBonus>2.5</maxPointsWithBonus>
  <reports>
    <report>
      <description>Vehicle changes direction to proper one after turning right</description>
      <successPercentage>1.0</successPercentage>
      <points>2.0</points>
      <maxPoints>2.0</maxPoints>
    </report>
    <report>
      <description>Vehicle blinks right when preparing to turn right</description>
      <successPercentage>0.0</successPercentage>
      <points>0.0</points>
      <maxPoints>0.0</maxPoints>
      <maxPointsWithBonus>0.5</maxPointsWithBonus>
    </report>
  </reports>
</report>
}}}

Using annotations is nice and straightforward but sometimes we cannot use them
effectively or we cannot them use at all e.g. we are not owners of the source code.
Therefore EAU support by default XML configuration of both: the test result and
the score definition. Test reports are supported in familiar Surefire report schema
which is format of generated test reports in usual maven build.
Score definition has its own internal definition.

Here is example of !VehicleTest01 test run result:

====Test result in Surefire XML format====

{{{
<testsuite name="acme.VehicleTest01" time="0.015">
	<testcase name="testTurnRight" classname="acme.VehicleTest01"
		time="0.0" />
	<testcase name="testPrepareRight" classname="acme.VehicleTest01"
		time="0.015">
		<failure>...very long stack trace...</failure>
	</testcase>
	<!-- other test cases -->
</testsuite>
}}}

And here is how would should score definition look like for this test:

====XML score definition====

{{{
<score>
	<suite name="acme.VehicleTest01">
		<check name="testTurnRight" points="2"
			message="Vehicle changes direction to proper one after turning right" />
		<check name="testPrepareRight" points="0.5" bonus="true"
			message="Vehicle blinks right when preparing to turn right">
			Vehicle must keep original direction when preparing to turn
		</check>
		<!-- other checks -->
	</suite>
</score>
}}}

Now you can evaluate it again only using XML files:

====Example command line run with acme.!VeteranCarTest with XML inputs====

{{{
java -cp "<path-to-eau-all.jar>;<path-to-your-xml-files>" eu.ebdit.eau.util.TextUI TEST-acme.VehicleTest01.xml VehicleTest01.score.xml
}}}

Resulting report should be the same as when you run test directly.

But how is this possible? How does the application know how should it handle 
the input?

EAU uses highly flexible and extendable mechanism which allows to collect
test results and score definitions from various sources. Sources for test results
can be JUnit test runs, Surefire test reports or, PMD reports etc. Also
score definitions may be various: annotated class, XML definition or definition
in some specific DSL.

The components which are responsible for collecting test results and score definitions
from input are called simply the collectors. Collectors need to implement generic interface
{{{eu.ebdit.eau.spi.Collector<T>}}} with generic parameter {{{<T>}}} set to {{{eu.ebdit.eau.Result}}}
or {{{eu.ebdit.eau.Score}}}.

Collectors can be discovered automatically over [http://java.sun.com/javase/6/docs/technotes/guides/jar/jar.html#Service%20Provider service provider interface]
defined by {{{eu.ebdit.eau.spi.Collector<T>}}} interface. If you run EAU application without specifying
concrete collector, input will be passed to each collector registered over SPI. If the collector can
collect some results or score definitions then it will return them otherwise it will return empty collection.

Note that you can also specify collectors manually. This is how should the previous example look like
using specified collectors. {{{-r}}} option is used to specify result collectors,
{{{-s}}} option is used to specify score definition collectors.

====Example command line run with specified collectors and XML inputs====

{{{
java -cp "<path-to-eau-all.jar>;<path-to-your-xml-files>" eu.ebdit.eau.util.TextUI -s eu.util.eau.util.XmlScoreParser -r eu.util.eau.util.XmlResultParser TEST-acme.VehicleTest01.xml VehicleTest01.score.xml
}}}

And this is how should look like example with JUnit test run.

====Example command line run with acme.!VeteranCarTest with specified collectors====

{{{
java -cp "<path-to-eau-all.jar>;<path-to-your-xml-files>" eu.ebdit.eau.util.TextUI -s eu.util.eau.util.ScoreAnnotationCollector -r eu.util.eau.junit.JUnitResultCollector acme.VeteranCarTest
}}}

If you know which collectors to use it is generally good idea to specify them directly. 
You can specify more score or result collectors by separating them by comma and 
putting them into double quotes.